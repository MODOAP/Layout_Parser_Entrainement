{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ModOAP_Creation_corpus_entrainement_LayoutParser .ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolrKtm9mCdp"
      },
      "source": [
        "# Création d'un corpus d'entraînement pour Layout Parser à partir de données OLR de documents Gallica\n",
        "\n",
        "Ce script permet de créer un corpus d'entraînement pour l'outil **ModOAP - Détection de mise en page (Entrainement)**, à partir d'annotations de mise en page (OLR) opérées sur des documents de presse numérisés en mode article et disponibles sur Gallica : [voir les corpus disponibles](https://api.bnf.fr/fr/documents-de-presse-numerises-en-mode-article) \n",
        "\n",
        "Le script nécessite de synchroniser un compte Google Drive sur lequel se trouve un dossier contenant les informations de mise en page téléchargées depuis https://api.bnf.fr/fr/documents-de-presse-numerises-en-mode-article (fichiers .xml contenus dans le dossier ocr)\n",
        "\n",
        "Ce script utilise le protocole de récupération d'images IIIF de Gallica : https://api.bnf.fr/fr/api-iiif-de-recuperation-des-images-de-gallica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpnTImAaRSZY",
        "cellView": "form"
      },
      "source": [
        "#@title  Imports et synchronisation du drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# chargement d'un google drive\n",
        "if not os.path.exists(\"/content/drive/MyDrive/\") :\n",
        "  drive.mount('/content/drive/')\n",
        "  \n",
        "import glob\n",
        "import gzip\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "import cv2\n",
        "try : \n",
        "  import xmltodict\n",
        "except ModuleNotFoundError : \n",
        "    \n",
        "  !pip install xmltodict\n",
        "  import xmltodict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cb_traitement(cb, ratio_height, ratio_width, image_id, ark) :\n",
        "  if isinstance(cb[\"TextBlock\"],dict) :\n",
        "    bloc_legende = {'@HPOS' : cb[\"TextBlock\"][\"@HPOS\"], \n",
        "                    '@VPOS' : cb[\"TextBlock\"][\"@VPOS\"], \n",
        "                    '@HEIGHT' : cb[\"TextBlock\"][\"@HEIGHT\"], \n",
        "                    '@WIDTH' : cb[\"TextBlock\"][\"@WIDTH\"], \n",
        "                    '@ID' : cb[\"TextBlock\"][\"@ID\"], \n",
        "                    '@LANG' : cb[\"TextBlock\"][\"@LANG\"]}\n",
        "\n",
        "    couples_ill_txt[ark+\"_\"+cb[\"@ID\"]] = {\"bloc_illustration\" : cb[\"Illustration\"],\n",
        "                                \"bloc_legende\" : bloc_legende ,\n",
        "                                \"ratio_height\" : ratio_height,\n",
        "                                \"ratio_width\" : ratio_width,\n",
        "                                \"image_id\" : image_id}\n",
        "  elif isinstance(cb[\"TextBlock\"],list) :\n",
        "    blocs_legende = []\n",
        "    for tb in cb[\"TextBlock\"] :\n",
        "      bloc_legende = {'@HPOS' : tb[\"@HPOS\"], \n",
        "                    '@VPOS' : tb[\"@VPOS\"], \n",
        "                    '@HEIGHT' : tb[\"@HEIGHT\"], \n",
        "                    '@WIDTH' : tb[\"@WIDTH\"], \n",
        "                    '@ID' : tb[\"@ID\"], \n",
        "                    '@LANG' : tb[\"@LANG\"]}\n",
        "      blocs_legende.append(bloc_legende)\n",
        "\n",
        "      couples_ill_txt[ark+\"_\"+cb[\"@ID\"]] = {\"bloc_illustration\" : cb[\"Illustration\"],\n",
        "                                    \"bloc_legende\" : blocs_legende,\n",
        "                                    \"ratio_height\" : ratio_height,\n",
        "                                    \"ratio_width\" : ratio_width,\n",
        "                                    \"image_id\" : image_id}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnKaUZzz_Xmw"
      },
      "source": [
        "**1. Préparer un dossier_corpus** contenant un sous-dossier par numéro de périodique contenant un fichier xml par page du document.\n",
        "\n",
        "Exemple : \n",
        "\n",
        "-dossier_corpus\n",
        "\n",
        "---465885\n",
        "*  X00000001.xml\n",
        "*  X00000002.xml\n",
        "*  X00000003.xml\n",
        "\n",
        "---475455\n",
        "*  X00000001.xml\n",
        "*  X00000002.xml\n",
        "*  X00000003.xml\n",
        "*  X00000004.xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgYMETHPmXs-",
        "cellView": "form"
      },
      "source": [
        "#@markdown  #### 2. Transformer le dossier_corpus en corpus d'entraînement pour Layout Parser\n",
        "\n",
        "dossier_corpus = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown  ##### Télécharge les pages du corpus au format jpg via IIF Gallica\n",
        "\n",
        "#@markdown  ##### Récupère les annotations des fichiers OCR pour créer un fichier au format COCO Json\n",
        "\n",
        "#@markdown  ##### Le dossier_corpus comprend au final deux sous-dossiers train et val, contenant chacun des pages du corpus au format jpg et un fichier d'annotation au format json\n",
        "\n",
        "fichiers_total = []\n",
        "print(\"Téléchargement des pages des documents et renommage des fichiers xml\")\n",
        "\n",
        "print()\n",
        "\n",
        "for dossier in tqdm(glob.glob(os.path.join(dossier_corpus,\"*\"))) : \n",
        "  files = glob.glob(os.path.join(dossier,\"*.xml\"))\n",
        "  with open(files[0],\"r\") as altin :\n",
        "    alto = altin.read()\n",
        "  altodic = xmltodict.parse(alto)\n",
        "  ark2 = altodic['alto'][\"Description\"][\"sourceImageInformation\"][\"fileIdentifier\"].split(\"/\")[-2]\n",
        "  for f in range(len(files)) :\n",
        "    url = \"https://gallica.bnf.fr/iiif/ark:/12148/{}/f{}/full/pct:20/0/native.jpg\".format(ark2,f+1)\n",
        "    nomfichier = ark2+\"_\"+\"view_\"+str(f)+\".jpg\"\n",
        "    cheminout = os.path.join(dossier,nomfichier)\n",
        "    try :\n",
        "      urllib.request.urlretrieve(url, cheminout)\n",
        "    except (HTTPError, URLError) as erreur:\n",
        "      print(str(erreur.reason)) \n",
        "\n",
        "  fichiersjpg = [fi for fi in glob.glob(os.path.join(dossier,\"*.jpg\"))]\n",
        "  fichiersxml = [fi for fi in glob.glob(os.path.join(dossier,\"*.xml\"))]\n",
        "  fichiersxml.sort(key=lambda fname: int(fname.split('/')[-1].split(\".\")[0][1:]))\n",
        "  fichiersjpg.sort(key=lambda fname: int(fname.split('_')[-1].split(\".\")[0]))\n",
        "  for e in fichiersxml :\n",
        "    fil_jpg = fichiersjpg[fichiersxml.index(e)].split(\"/\")[-1][:-4]+\".xml\"\n",
        "    chemin = \"/\".join(e.split(\"/\")[:-1])\n",
        "    os.rename(e,os.path.join(chemin,fil_jpg))\n",
        "  \n",
        "  fichiers_total = fichiers_total + [fi for fi in glob.glob(os.path.join(dossier,\"*.xml\"))] + fichiersjpg\n",
        "\n",
        "  print(len(fichiers_total))\n",
        "\n",
        "print(\"Séparation Test/Val\")\n",
        "fichiers_total.sort()\n",
        "index_split= int(len(fichiers_total) * 80/100) \n",
        "if (index_split % 2) != 0:  \n",
        "   index_split += 1\n",
        "train = fichiers_total[:index_split]\n",
        "val = fichiers_total[index_split:]\n",
        "print(\"Train : {} fichiers\".format(len(train)))\n",
        "\n",
        "if not os.path.exists(os.path.join(dossier_corpus,\"test\")) :\n",
        "  os.makedirs(os.path.join(dossier_corpus,\"test\"))\n",
        "if not os.path.exists(os.path.join(dossier_corpus,\"val\")) :\n",
        "  os.makedirs(os.path.join(dossier_corpus,\"val\"))\n",
        "for fichier in tqdm(train) :\n",
        "  shutil.move(fichier, os.path.join(dossier_corpus,\"test\"))\n",
        "\n",
        "print()\n",
        "print(\"Val : {} fichiers\".format(len(val)))\n",
        "\n",
        "for fichier in tqdm(val) :\n",
        "  shutil.move(fichier, os.path.join(dossier_corpus,\"val\"))\n",
        "\n",
        "for dossier in glob.glob(os.path.join(dossier_corpus,\"*\")):\n",
        "  if not \"val\" in dossier and not \"test\" in dossier : \n",
        "    shutil.rmtree(dossier)\n",
        "\n",
        "#############################################################################################################################\n",
        "# Récupération des annotations\n",
        "\n",
        "dossiers_images = [f for f in glob.glob(os.path.join(dossier_corpus,\"*\"))]\n",
        "\n",
        "for dossier_images in dossiers_images :\n",
        "  print(\"dossier : \", dossier_images)\n",
        "\n",
        "  ######### Création de liste_images #############\n",
        "\n",
        "  ######### Création de couples_ill_txt #############\n",
        "  liste_images = []\n",
        "  couples_ill_txt = {}\n",
        "\n",
        "  fichiers_pages_jpg = [p for p in glob.glob(os.path.join(dossier_images,\"*.jpg\"))]\n",
        "  #fichiers_pages_jpg.sort(key=lambda fname: int(fname.split('_')[-1].split(\".\")[0]))\n",
        "\n",
        "  print(\"fichiers : \", fichiers_pages_jpg)\n",
        "\n",
        "  image_id = 0\n",
        "\n",
        "  for f in tqdm(fichiers_pages_jpg) :\n",
        "      ark = f.split(\"/\")[-1].split(\"_\")[0]\n",
        "      \n",
        "      img = cv2.imread(f)\n",
        "      height = img.shape[0]\n",
        "      width = img.shape[1]\n",
        "      file_name = f.split(\"/\")[-1]\n",
        "      image_id += 1\n",
        "      dico_image = {\"file_name\" : file_name,\n",
        "                    \"height\" : height,\n",
        "                    \"width\" : width,\n",
        "                    \"id\" : image_id}\n",
        "      liste_images.append(dico_image)\n",
        "\n",
        "\n",
        "      with open(f.replace(\".jpg\",\".xml\"), \"r\") as alt :\n",
        "        alto = alt.read()\n",
        "        altodic = xmltodict.parse(alto)\n",
        "        height_ocr = int(altodic[\"alto\"][\"Layout\"][\"Page\"][\"@HEIGHT\"])\n",
        "        width_ocr = int(altodic[\"alto\"][\"Layout\"][\"Page\"][\"@WIDTH\"])\n",
        "        \n",
        "        ratio_height = height / height_ocr\n",
        "        ratio_width = width / width_ocr\n",
        "\n",
        "        try :\n",
        "          if isinstance(altodic['alto']['Layout']['Page']['PrintSpace']['ComposedBlock'], dict) :\n",
        "            cb = altodic['alto']['Layout']['Page']['PrintSpace']['ComposedBlock']\n",
        "            cb_traitement(cb, ratio_height, ratio_width, image_id, ark)\n",
        "          elif isinstance(altodic['alto']['Layout']['Page']['PrintSpace']['ComposedBlock'], list) :\n",
        "            for cb in altodic['alto']['Layout']['Page']['PrintSpace']['ComposedBlock'] :\n",
        "              cb_traitement(cb, ratio_height, ratio_width, image_id, ark)\n",
        "          else : print(\"PROBLEME\")\n",
        "        except KeyError : pass\n",
        "\n",
        "\n",
        "  #  Extraction des annotations et enregistrement au format COCO\n",
        "\n",
        "  chemin_sauvegarde = os.path.join(dossier_images,\"annotations_COCO.json\")\n",
        "  print(\"sauvegarde dans : \", chemin_sauvegarde)\n",
        "\n",
        "  ######### Création de anno #############\n",
        "\n",
        "  anno = []\n",
        "  n = 1\n",
        "  for couple in tqdm(couples_ill_txt) :\n",
        "\n",
        "    image_id = couples_ill_txt[couple][\"image_id\"]\n",
        "\n",
        "    if isinstance(couples_ill_txt[couple][\"bloc_illustration\"], dict) :\n",
        "\n",
        "      hposI = int(int(couples_ill_txt[couple][\"bloc_illustration\"][\"@HPOS\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "      vposI = int(int(couples_ill_txt[couple][\"bloc_illustration\"][\"@VPOS\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "      heightI = int(int(couples_ill_txt[couple][\"bloc_illustration\"][\"@HEIGHT\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "      widthI = int(int(couples_ill_txt[couple][\"bloc_illustration\"][\"@WIDTH\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "\n",
        "      segmentation = [hposI, vposI,hposI + widthI,vposI,hposI + widthI,vposI+heightI, hposI,vposI+heightI]\n",
        "      bbox = [hposI, vposI, widthI ,heightI]\n",
        "      area = heightI * widthI\n",
        "      \n",
        "      dico_anno = {\"segmentation\" : [segmentation], \"area\" : area , \"iscrowd\" : 0, \"image_id\" : image_id, \"bbox\" : bbox, \"category_id\" : 1, \"id\": n}\n",
        "      n+= 1\n",
        "      anno.append(dico_anno)\n",
        "\n",
        "    elif isinstance(couples_ill_txt[couple][\"bloc_illustration\"], list) :\n",
        "      for illus in couples_ill_txt[couple][\"bloc_illustration\"] :\n",
        "        hposI = int(int(illus[\"@HPOS\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "        vposI = int(int(illus[\"@VPOS\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "        heightI = int(int(illus[\"@HEIGHT\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "        widthI = int(int(illus[\"@WIDTH\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "\n",
        "        segmentation = [hposI, vposI,hposI + widthI,vposI,hposI + widthI,vposI+heightI, hposI,vposI+heightI]\n",
        "        bbox = [hposI, vposI,widthI,heightI]\n",
        "        area = heightI * widthI\n",
        "        \n",
        "        dico_anno = {\"segmentation\" : [segmentation], \"area\" : area , \"iscrowd\" : 0, \"image_id\" : image_id, \"bbox\" : bbox, \"category_id\" : 1, \"id\": n}\n",
        "        n+= 1\n",
        "        anno.append(dico_anno)\n",
        "\n",
        "    if isinstance(couples_ill_txt[couple][\"bloc_legende\"], dict) :\n",
        "\n",
        "      hposL = int(int(couples_ill_txt[couple][\"bloc_legende\"][\"@HPOS\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "      vposL = int(int(couples_ill_txt[couple][\"bloc_legende\"][\"@VPOS\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "      heightL = int(int(couples_ill_txt[couple][\"bloc_legende\"][\"@HEIGHT\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "      widthL = int(int(couples_ill_txt[couple][\"bloc_legende\"][\"@WIDTH\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "\n",
        "      segmentation = [hposL, vposL,hposL + widthL,vposL,hposL + widthL,vposL+heightL, hposL,vposL+heightL]\n",
        "      bbox = [hposL, vposL, widthL, heightL ]\n",
        "      area = heightL * widthL\n",
        "\n",
        "      dico_anno = {\"segmentation\" : [segmentation], \"area\" : area , \"iscrowd\" : 0, \"image_id\" : image_id, \"bbox\" : bbox, \"category_id\" : 2, \"id\": n}\n",
        "      n+= 1\n",
        "      anno.append(dico_anno)\n",
        "\n",
        "\n",
        "    elif isinstance(couples_ill_txt[couple][\"bloc_legende\"], list) :\n",
        "      for tb in couples_ill_txt[couple][\"bloc_legende\"] :\n",
        "\n",
        "\n",
        "        hposL = int(int(tb[\"@HPOS\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "        vposL = int(int(tb[\"@VPOS\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "        heightL = int(int(tb[\"@HEIGHT\"]) * couples_ill_txt[couple][\"ratio_height\"])\n",
        "        widthL = int(int(tb[\"@WIDTH\"]) * couples_ill_txt[couple][\"ratio_width\"])\n",
        "\n",
        "        segmentation = [hposL, vposL,hposL + widthL,vposL,hposL + widthL,vposL+heightL, hposL,vposL+heightL]\n",
        "        bbox = [hposL, vposL, widthL,heightL]\n",
        "        area = heightL * widthL\n",
        "\n",
        "        dico_anno = {\"segmentation\" : [segmentation], \"area\" : area , \"iscrowd\" : 0, \"image_id\" : image_id, \"bbox\" : bbox, \"category_id\" : 2, \"id\": n}\n",
        "        n+= 1\n",
        "        anno.append(dico_anno)\n",
        "  ######### Création du Json final #############\n",
        "\n",
        "  info = {}\n",
        "  licenses = {}\n",
        "  images = liste_images\n",
        "  categories = [{'supercategory': 'illustration', 'id': 1, 'name': 'illustration'}, {'supercategory': 'legende', 'id': 2, 'name': 'legende'} ]\n",
        "\n",
        "  cocofinal = {\"info\" : info,\n",
        "              \"licences\" : licenses,\n",
        "              \"images\" : images,\n",
        "              \"categories\" : categories, \n",
        "              \"annotations\" : anno} \n",
        "\n",
        "  with open(chemin_sauvegarde, \"w\") as jout:\n",
        "    json.dump(cocofinal, jout )\n",
        "\n",
        "  for xml in [f for f in glob.glob(os.path.join(dossier_images,\"*.xml\"))]:\n",
        "    os.remove(xml)\n",
        "print()\n",
        "print(\"Le dossier {} est prêt pour l'entraînement\".format(dossier_corpus))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}